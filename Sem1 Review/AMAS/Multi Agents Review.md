[TOC]

# Week2 Embedded Agents

Agents are embedded in an environment, meaning that an agent affects and is affected by the environment. An agent experiences its environment through sensors an acts on its environment through effectors. 

## 2.1 Math revision

- Power Set: The power set of a set A is the set of all subsets of A. It is written as $2^A$
  - $A=\{xâˆ£0<x<7$ and $x$ is odd$\}$. A={1,3,5} hence $2^A$={âˆ…,{1},{3},{5},{1,3},{1,5},{3,5},{1,3,5}}

## 2.2 Accessible and inaccessible environments

- An accessible environment is one in which the agent can obtain complete, accurate, up-to-date information about the environmentâ€™s state

- Most moderately complex environments (including, for example, the everyday physical world and the Internet) are inaccessible.

Example:

- Accessible: Chess
- Inacessible: Stock market

## 2.3 Deterministic and non-deterministic environments

- A deterministic environment is one in which every action has a single guaranteed effect â€” there is no uncertainty about the state that will result from performing an action.
- The physical world can to all intents and purposes be regarded as non-deterministic.

## 2.4 Static and dynamic environments

- A static environment is one in which the only changes to the environment are those caused by actions made by the agent.å¯¹ç¯å¢ƒçš„å”¯ä¸€æ›´æ”¹æ˜¯ç”±agentæ‰§è¡Œçš„æ“ä½œå¼•èµ·çš„æ›´æ”¹ã€‚

- A dynamic environment is one that has other processes and/or agents operating on it, and so changes in ways beyond the agentâ€™s control. ä¸ä»…ä»…æ˜¯æœ‰ä¸€ä¸ªagentåœ¨å†³ç­–ï¼Œå…¶ä»–agentsåœ¨å†³ç­–æ—¶ä¼šäº’ç›¸å½±å“

## 2.5 Formal specification of an embedded agent

$Env = <E, e_0, \pi>$

## 2.6 Utility Functions

Utility functions allow an agent to understand how "good" an outcome is. 



# Week3:Deductive,reactive and hybrid reasoning agents

- Deductive reasoning agents, which have an explicitly model of the world represented in logical formulas and reason using logical deduction. 
  - Concurrent MetateM, a specific language for specifying deductive reasoning agents.

- Reactive agents, which do not have an explicit model of the world that they reason with, but instead their behaviour is driven by a set of "stimulus -> action" rules.
  - The subsumption architecture, probably the most well known reactive agent architecture.

- Hybrid agents, which combine a purely reactive layer with higher-level reasoning layers.
  - We'll look at TouringMachines and InteRRaP as two examples of these. 


## 3.1 Deductive Agents

### 3.1.1 Concurrent MetateM

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211225184017386.png" alt="image-20211225184017386" style="zoom:40%;" />

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211225184524543.png" alt="image-20211225184524543" style="zoom:40%;" />

E.g:

$student(you)$  $\upsilon$ $graduate(you)$ -- at some point in the future you will graduate, until then you will be a student. ç›´åˆ°æ¯•ä¸šå‰ä½ ä¸€ç›´æ˜¯å­¦ç”Ÿ

$$title(me,dr)$$ $S$ ğ‘ğ‘¤ğ‘ğ‘Ÿğ‘‘ğ‘’ğ‘‘(ğ‘šğ‘’,ğ‘â„ğ‘‘) -- at some point in the past I was awarded a PhD and my title has been Dr ever since then. ä»æˆäºˆæˆ‘PHDå­¦ä½ä»¥åï¼Œæˆ‘ä¸€ç›´éƒ½æ˜¯Dr

$Â¬ğ‘“ğ‘Ÿğ‘–ğ‘’ğ‘›ğ‘‘ğ‘ (ğ‘šğ‘’,ğ‘¦ğ‘œğ‘¢)$ $W $ $ğ‘ğ‘ğ‘œğ‘™ğ‘œğ‘”ğ‘–ğ‘ ğ‘’(ğ‘¦ğ‘œğ‘¢,ğ‘šğ‘’)$ -- we will never be friends unless you apologise to me.

$title(you,dr)$ $Z$ $awarded(you,phd)$ -- if it's true that you've been awarded a PhD in the past, then your title has been Dr ever since then.

## 3.2 Reactive Agents

 ä¸å†ä½¿ç”¨åƒMetaMä¸­é‚£æ ·çš„ç¬¦å·æ¥è¡¨ç¤ºè¡Œä¸ºï¼Œåªæ˜¯é€šè¿‡å“åº”ç¯å¢ƒè€Œäº§ç”Ÿçš„å„ç§ç®€å•è¡Œä¸ºçš„ç›¸äº’ä½œç”¨äº§ç”Ÿæ™ºèƒ½è¡Œä¸ºã€‚

### 3.2.1 subsumption architecture

subsumption architecture - the most famous reactive agent architecture

æŠŠè¡Œä¸ºç»„ç»‡æˆç­‰çº§å±‚æ¬¡ç»“æ„ï¼Œç­‰çº§å±‚çº§ç»“æ„ä¸­çš„ä½å±‚å¯ä»¥æŠ‘åˆ¶é«˜å±‚ï¼Œå±‚æ¬¡è¶Šä½ä¼˜å…ˆåº¦è¶Šé«˜ï¼Œ**å…¶ä¸»è¦æ€æƒ³å°±æ˜¯å±‚æ¬¡è¶Šé«˜è¡¨ç¤ºçš„è¶ŠæŠ½è±¡çš„è¡Œä¸º**ä¸¾ä¾‹æ¥è¯´ï¼Œæœ‰äººå¸Œæœ›ç§»åŠ¨æœºå™¨ï¼Œäººæœ‰èº²é¿éšœç¢çš„è¡Œä¸ºã€‚è¿™æ˜¾ç„¶è¦ç»™èº²é¿éšœç¢ç‰©èµ‹äºˆä¸€ä¸ªé«˜çš„ä¼˜å…ˆçº§ã€‚

ä¸¾ä¾‹ï¼šç«æ˜Ÿè½¦Steels' experiments with the Mars explorer system

å±€é™æ€§ï¼š

- ç”±äºçº¯ååº”å¼ Agent æŒ‰ç…§å±€éƒ¨ä¿¡æ¯ï¼ˆå³å…³äºAgentå½“å‰çŠ¶æ€çš„ä¿¡æ¯ï¼‰åšå‡ºå¤®ç­–ï¼Œå¾ˆéš¾æƒ³åƒè¿™ç§å†³ç­–æ–¹æ³•èƒ½è€ƒè™‘éå±€éƒ¨ä¿¡æ¯
- å¿…é¡»ç»è¿‡è´¹åŠ›è´¹æ—¶æ‰èƒ½åˆ›é€ å‡ºçº¯ååº”å¼ Agent
- Agentså¯ç”¨çš„è¡Œä¸ºè¶Šå¤šï¼Œç†è§£æˆ–é¢„æµ‹å®ƒä»¬çš„è¡Œä¸ºå°±è¶Šå›°éš¾ï¼Œå› ä¸ºä¸åŒå±‚ä¹‹é—´çš„ç›¸äº’ä½œç”¨çš„åŠ¨æ€å˜å¾—å¤ªå¤æ‚è€Œæ— æ³•ç†è§£ã€‚

## 3.3 Hybrid Agents



# Week4: Practical Reasoning Agents

## 2.1 Delibration and Means-ends reasoning

- **Deliberation**: deciding what state of affairs the agent wants to achieve; the results of deliberation are referred to as the agent's **intentions**. ( **weighing up** the options available to me and **deciding** which ones to commit to as intentions.)

- **Means-ends reasoning**: deciding how to achieve these states of affairs. (Once I've decided on my intention, I need to work out what I'm going to do to try to achieve this.)
- **Intentions**: We have seen that the output of the agent's deliberation are **intentions**. If an agent has an intention X this means is intends to (try) to bring about the state of affairs X.

## 2.2 Resource bounded reasoning and calculative rationality

calculative rationality

E.g:

Is the following statement true or false?

An agent with the property of calculative rationality that is situated in a static environment is guaranteed to always make optimal decisions.

TRUE

If an agent has the property of calculative rationality, the decisions it makes are guaranteed to be optimal according to the information it had available when it started making its decision. If the environment is static, this means that the only way the world can change is if the agent itself performs some action. This means that the world is guaranteed not to change during the time the agent is deciding what to do, and so a decision that is optimal according to the information available when the decision making process started will still be optimal when the agent finishes making its decision.

## 2.3 BDI agents

BDI stands for **Beliefs, Desires, Intentions**

Based on an agent's beliefs, it has a set of desires. Once an agent commits to trying to achieve one of its desires, this desire becomes an intention. 

E.g: If my workload eases then I might decide it is feasible to visit my family and commit to this as an intention. Once I have committed to this as an intention, we expect (as discussed previously) that: I will invest some effort in trying to achieve this;

### 2.3.1 BDI V1

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211226025455604.png" alt="image-20211226025455604" style="zoom:40%;" />

- When an agent receives a new percept from the environment, it updates its beliefs to take this new information into account using its belief revision function: $brf()$.
- $options()$, which takes as input the agent's current beliefs and intentions and returns a set of desires;
- $filter()$ function, which takes as input the agent's current beliefs, desires and intentions and returns the "best" options to commit to as its new intentions.
- $plan()$ Function,  which takes the agent's current beliefs and intentions, and returns a plan for the agent to follow in order to try to achieve its intentions. 
- Means-ends reasoning is concerned with working out what the agent is going to do to try to achieve its intentions. This is what the $plan()$ function does: determines a plan the agent is going to employ to try to achieve its intentions.
- **Deliberation is a two step process**. First the agent identifies its options (line 6) and then the agent filters these to select its intentions (line 7). Deliberation therefore takes place over lines 6 and 7.

### 2.3.2 BDI V2

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211226030123046.png" alt="image-20211226030123046" style="zoom:40%;" />

Version 2 of our agent control loop allows the agent to execute one action from its plan at a time, pausing to observe the environment after each action and reconsider whether its plan is still appropriate based on its new beliefs.

- $empty(Ï€)$ is true if and only if the plan Ï€ is empty.
- $â„ğ‘’ğ‘ğ‘‘(ğœ‹)$ returns the first action of the plan Ï€.
- $ğ‘¡ğ‘ğ‘–ğ‘™(ğœ‹)$ returns the remainder of the plan Ï€ once the first action of ğœ‹ has been removed.
- $ğ‘ ğ‘œğ‘¢ğ‘›ğ‘‘(ğœ‹,ğ¼,ğµ)$ is true if and only if, according to the agent's beliefs ğµ, the agent can expect the plan ğœ‹ to achieve its intentions ğ¼. 



### 2.3.3 BDI V3

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211226030927828.png" alt="image-20211226030927828" style="zoom:40%;" />

In version 2 of the control loop, the only time the agent ever stops to consider its intentions is when it has completed executing its plan. We will now adapt the control loop so that the agent is able to adjust its intentions if it is appropriate to do so. For this we need to introduce the following predicates.

- $succeeded(I,B)$ is true if and only if, based on its beliefs B, the agent believes it has achieved its intentions ğ¼.
- $impossible(I,B)$ is true if and only if, based on its beliefs B, the agent believes it is impossible to achieve its intentions ğ¼

If the agent believes that its intentions are either impossible or already achieved, there is no point in the agent persisting in trying to achieve those intentions. 

Agents using version 3 of the BDI control loop stop and deliberate again in order to reconsider their intentions after every action they execute (lines 15-16 of control loop version 3).



### 2.3.4 BDI V4 (Meta-level)

æ…æ€çš„è¿‡ç¨‹éœ€è¦èŠ±è´¹å¤§é‡çš„æ—¶é—´ï¼Œå½“ä¸€ä¸ªagentåœ¨æ…æ€æ—¶ï¼Œå‘¨å›´çš„ç¯å¢ƒä¹Ÿåœ¨æ”¹å˜ï¼Œå¯èƒ½ä¼šæå‡ºæ²¡æœ‰å…³ç³»çš„æ–°å½¢æˆçš„æ„å›¾

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211226031704202.png" alt="image-20211226031704202" style="zoom:40%;" />



-  $reconsider(I,B)$ is true if and only if, given its beliefs B, the agent believes it is appropriate to reconsider its intentions ğ¼.

There is an important implicit assumption here: the cost of determining whether **reconsider(I,B) is true must be less than the cost of the actual deliberation** process itself (i.e., performing functions options and filters). If this were not the case, then the agent might as well go ahead and do the deliberation, since this would be less costly than trying to decide whether it should deliberate or not.



E.gï¼š

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211226033608068.png" alt="image-20211226033608068" style="zoom:50%;" />

agentsæ…æ€ä¹‹åå°±ä¼šæ”¹å˜intentionï¼Œå› ä¸ºdeliberationæ˜¯ä¸¤éƒ¨åˆ†ç»„æˆï¼ˆ **weighing up** the options available to me and **deciding** which ones to commit to as intentionsï¼‰ã€‚å½“ä¸”ä»…å½“Agenté€‰æ‹©é•‡æ€çš„æ—¶å€™ï¼Œä½¿Agent æ”¹å˜æ„å›¾ï¼Œä¸¤æ•°reconsider (â€¦)çš„è¡Œä¸ºæ˜¯æœ€ä¼˜çš„ã€‚å¯¹äº
Agent é€‰æ‹©äº†æ…æ€è¿‡ç¨‹ï¼Œä½†æ²¡æœ‰æ”¹äº¤æ„å›¾æ—¶ï¼Œåˆ™åœ¨æ…æ€è¿‡ç¨‹ä¸­èŠ±è´¹çš„åŠªåŠ›å°±æ˜¯æµªè´¹ã€‚åŒæ ·ï¼Œå¦‚æœ Agentåº”è¯¥æ”¹å˜æ„å›¾ï¼Œä½†æ˜¯æ²¡æœ‰èƒ½æ”¹å˜å¤±è´¥äº†ï¼Œåˆ™èŠ±è´¹åœ¨å®ç°æ„å›¾ä¸Šçš„åŠªåŠ›ä¹Ÿæ˜¯æµªè´¹çš„ã€‚å› æ­¤1å’Œ4æœ€ä¼˜





# Week5: communication, coordination and cooperation

## 5.1 Communication

Communication acts:

- The **locutionary act**: this is the act of making the utterance.
- The **llocutionary act**: the conveying of the speaker's intentions to the hearer.
- The **perlocution**: the effect achieved by the speech act.

For example, if I utter the speech act "Please make me some tea":

- The locutionary act is me saying "Please make me some tea".
- The illocutionary act is that I have requested you to make me some tea.
- The perlocution depends on how you respond to my request. I hope that you will make me some tea, but you might not.

### 5.1.1 KQML

Autonomous heterogeneous agents in a multi-agent system require a shared **agent communication language** (ACL) so they can communicate with one another. KQMLå±äºACLçš„ä¸€ç§



### 5.1.2 FIPA

åœ¨FIPA ACLä¸­ï¼ŒæŠŠå‘é€agentå¿…é¡»è¦æ»¡è¶³çš„æ¡ä»¶å«å¯è¡Œæ€§å‰æï¼ˆFeasibility Preconditionï¼‰ï¼ŒæŠŠAgentå¸Œæœ›é€šè¿‡æ‰§è¡Œé€šä¿¡è¡ŒåŠ¨åæ‰€å–å¾—çš„æ•ˆæœç§°ä¸ºç†æ€§æ•ˆæœï¼ˆRational Effectï¼‰

the intended effect of each performative is defined (the rational effect): the receiver of a message understands that this is what the sender wishes to achieve. 

FIPA ACL. è¯­ä¹‰è¿˜è§„å®šäº†å‘é€æ–¹è®¤ä¸ºåº”å½“æ˜¯çœŸçš„æ¡ä»¶.è€ƒè™‘ä¸€ä¸ªé€šä¿¡è¡ŒåŠ¨$(i,inform (j, \alpha)ã€‰$ï¼Œåœ¨è¿™é‡Œï¼ŒInform çš„å†…å®¹æ˜¯å‘½é¢˜$\alpha$ï¼Œå®ƒçš„å«ä¹‰æ˜¯ Agent ié€šçŸ¥ Agent j å‘½é¢˜$\alpha$ä¸ºçœŸã€‚ æ ¹æ® FIPA ACL. çš„è¯­ä¹‰,inform çš„Feasibility Precondition å°±æ˜¯Agent iè®¤ä¸ºå‘½é¢˜$\alpha$ä¸ºçœŸ,å¹¶ä¸”ç›¸ä¿¡ Agentjæ²¡æœ‰å…³äº$\alpha$çš„ä»»ä½•çŸ¥è¯†ã€‚Inform çš„Rational Effectå°±æ˜¯ Agent i æƒ³è®© Agent jç›¸ä¿¡å‘½é¢˜$\alpha$ä¸ºçœŸ.

E.g:

If we see an agent send a FIPA request message then we can be sure that the feasibility precondition is true? FALSE!!!

è§£é‡Šï¼šThe feasibility precondition is given in terms of the sender's mental state (i.e., the sender must believe that the receiver can perform the action, and they must not believe that the receiver already intends to perform the action). Agents are autonomous, independent entities and we cannot (in general) access their private mental state. This means that we can't be sure, when an agent sends a request message, that the relevant feasibility precondition holds. For example, if the sender is a malicious agent, it may in fact believe that the receiver cannot perform the action, but nevertheless makes the request as a way of disrupting the receiver (by taking up its attention with the message). What this means in general is that we cannot be sure that agents are conforming to the semantics of the FIPA ACL.

### 5.1.3 Social commitments

By giving a clear semantics for an ACL, we aim to make the agents' reasoning process easier. If an agent ag1 is conforming to the semantics of the FIPA ACL, then this means that an agent ag2 receiving a message from ğ‘ğ‘”1ag1 can know something about ag1's mental state. But we have just seen that we cannot be sure if an agent is conforming to the FIPA ACL semantics, exactly because they are defined in terms of the agent's mental state. This is a challenge for ACLs that is not yet resolved. One approach to address this is to define ACL semantics based on **social commitments.**



## 5.2 Cooperation

**Cooperative Distributed Problem Solving** (CDPS)



Cooperative Distributed Problem Solving involves these steps:

- Problem decomposition: dividing the problem into smaller tasks for distribution among agents.
-  Subproblem allocation: determining which subproblems to allocate to which agents.
- Subproblem solution: where the agents solve the individual subproblems that they have been allocated.
- Subproblem synthesis: where solutions to individual subproblems are integrated into an overall solution.

### 5.2.1 Contract Net protocol

which addresses the second stage of Cooperative Distributed Problem Solving (CDPS): sub-problem allocation. 

åœ¨åˆåŒç½‘åè®®ä¸­ï¼Œæ‰€æœ‰Agent å¯ä»¥å½’çº³ä¸ºä¸¤ç§è§’è‰²ï¼šç®¡ç†è€…å’Œæ‰¿åŒ…å•†ã€‚å…¶ä¸­ï¼Œç®¡ç†è€…çš„èŒè´£åŒ…æ‹¬ï¼šå¯¹æ¯ä¸€ä¸ªå¾…æ±‚è§£ä»»åŠ¡å»ºç«‹
ä»»åŠ¡é€šçŸ¥ä¹¦ï¼Œå°†ä»»åŠ¡é€šçŸ¥ä¹¦å‘é€ç»™æœ‰å…³çš„æ‰¿åŒ…å•† Agent ï¼›æ¥æ”¶å¹¶è¯„ä¼°æ¥è‡ªæ‰¿åŒ…å•†çš„æŠ•æ ‡ï¼›ä»æŠ•æ ‡ä¸­é€‰æ‹©æœ€åˆé€‚çš„æ‰¿åŒ…å•†ï¼Œä¸ä¹‹å»ºç«‹
åˆåŒï¼›ç›‘ç£ä»»åŠ¡çš„å®Œæˆï¼Œå¹¶ç»¼åˆç»“æœã€‚æ‰¿åŒ…å•†çš„èŒè´£åŒ…æ‹¬ï¼šæ¥æ”¶ç›¸å…³ä»»åŠ¡é€šçŸ¥ä¹¦ï¼šæ ¹æ®è‡ªå·±çš„èƒ½åŠ›åˆ¤æ–­æ˜¯å¦æ¥å—ä»»åŠ¡ï¼Œä¸æ¥å—å‘é€æ‹’æ ‡é€šçŸ¥ï¼Œå¦åˆ™å‘é€æŠ•æ ‡é€šçŸ¥ï¼šå¦‚æœæŠ•æ ‡è¢«æ¥å—ï¼ŒæŒ‰åˆåŒæ‰§è¡Œåˆ†é…ç»™è‡ªå·±çš„ä»»åŠ¡ï¼Œå‘ç®¡ç†è€…å‘é€æ±‚è§£ç»“æœã€‚

åœ¨åˆåŒç½‘åä½œæ–¹æ³•ä¸­ï¼Œä¸éœ€è¦é¢„å…ˆè§„å®š Agent çš„è§’è‰²ï¼Œä»»ä½•Agenté€šè¿‡å‘å¸ƒä»»åŠ¡é€šçŸ¥è€Œæˆä¸ºç®¡ç†è€…ï¼›ä»»ä½• Agenté€šè¿‡åº”ç­”ä»»åŠ¡é€šçŸ¥è€Œæˆä¸ºæ‰¿åŒ…å•†ã€‚ç³»ç»Ÿä¸­çš„æ¯ä¸€å¾…æ±‚è§£ä»»åŠ¡ï¼Œç”±æ‰¿æ‹…è¯¥ä»»åŠ¡çš„Agent è´Ÿè´£å®Œæˆã€‚å½“è¯¥ Agent æ— æ³•ç‹¬ç«‹å®Œæˆè¯¥ä»»åŠ¡æ—¶ï¼Œå®ƒå°±å°†å±¥è¡Œç®¡ç†è€…èŒè´£ï¼Œä¸ºè¯¥ä»»åŠ¡å‘é€ä»»åŠ¡é€šçŸ¥ä¹¦ï¼šç„¶åä»è¿”å›çš„æŠ•æ ‡ä¸­é€‰æ‹©â€˜æœ€åˆé€‚â€çš„ Agentï¼Œå°†ä»»åŠ¡åˆ†é…ç»™æ­¤Agentï¼Œå»ºç«‹ç›¸åº”çš„åˆåŒã€‚



## 5.3 Coordinate

åœ¨åˆä½œä¸­ï¼Œè¦è®¨è®ºçš„ä¸»è¦é—®é¢˜å°±æ˜¯åè°ƒé—®é¢˜ã€‚åè°ƒé—®é¢˜æ˜¯æŒ‡å¦‚ä½•ç®¡ç† AgentåŠ¨ä½œä¹‹é—´çš„å†…éƒ¨ä¾èµ–å…³ç³»ã€‚å¦‚æœ Agent å‚åŠ çš„åŠ¨ä½œä¸­å­˜åœ¨ä»¥ä»»ä½•æ–¹å¼çš„ç›¸äº’ä½œç”¨ï¼Œé‚£ä¹ˆä¸€å®šçš„åè°ƒæœºåˆ¶æ˜¯å¿…éœ€çš„ã€‚ä¸¤ä¸ªåŠ¨ä½œä¹‹é—´æ˜¯å¦‚ä½•è¿›è¡Œç›¸äº’ä½œç”¨çš„ï¼Ÿå¯ä»¥è€ƒæ„Ÿä¸‹é¢ç°å®ä¸–ç•Œä¸­çš„ä¾‹å­ï¼š

- ä½ å’Œæˆ‘éƒ½æ‰“ç®—å•†å¼€æˆ¿é—´ï¼Œå¹¶ä¸”æˆ‘ä»¬å„è‡ªå‘é—¨èµ°å»ï¼Œè€Œé—¨åªå…è®¸æˆ‘ä»¬ä¸€ä¸ªäººè¿‡å»ã€‚æˆ‘æœ‰ç¤¼è²Œåœ°å…è®¸ä½ å…ˆç¦»å¼€ã€‚
  åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çš„æ´»åŠ¨éœ€è¦åè°ƒï¼Œå› ä¸ºè¿™é‡Œåªæœ‰ä¸€ä¸ªèµ„æºï¼ˆé—¨ï¼‰ï¼Œè€Œæˆ‘ä»¬éƒ½éœ€è¦ä½¿ç”¨è¯¥èµ„æºï¼Œä½†æ˜¯åœ¨åŒä¸€æ—¶é—´åªå…è®¸ä¸€ä¸ªäººä½¿ç”¨ã€‚
- æˆ‘æ‰“ç®—æäº¤ä¸€ä»½åŠ©å­¦é‡‘ç”³è¯·ä¹¦ï¼Œä½†æ˜¯ä¸ºäº†åšè¿™ä»¶äº‹ï¼Œéœ€è¦ä½ çš„ç­¾å­—ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘æäº¤åŠ©å­¦é‡‘ç”³è¯·ä¹¦çš„åŠ¨ä½œä¾èµ–äºä½ ç­¾å­—çš„åŠ¨ä½œ
  --é™¤éä½ çš„åŠ¨ä½œå®Œæˆï¼Œæˆ‘ä¸èƒ½å®Œæˆæˆ‘çš„åŠ¨ä½œã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘çš„åŠ¨ä½œä¾èµ–äºä½ çš„åŠ¨ä½œã€‚
- æˆ‘ä»ç½‘ç»œä¸Šå¾—åˆ°ä¸€ä»½æ–‡ä»¶çš„è½¯æŒè´ã€‚æˆ‘çŸ¥é“ä½ ä¹Ÿå¯¹è¿™ä»½æŠ¥å‘Šæ„Ÿç«è¶£ï¼Œè¿™æ ·æˆ‘ä¸»åŠ¨å¤å°
  ä¸€ä»½æŠ¥å‘Šï¼Œå¹¶ç»™ä½ ä¸€ä»½æ‹·è´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„åŠ¨ä½œä¸éœ€è¦ä¸¥æ ¼çš„åè°ƒ-â€”å› ä¸ºè¿™ä»½æŠ¥å‘Šåœ¨ç½‘ä¸Šå¯ä»¥å…è´¹è·å¾—ï¼Œä½ ä¹Ÿå¯èƒ½ä¸‹è½½å¹¶æ‰“å°ã€‚ä½†æ˜¯é€šè¿‡æˆ‘ä¸»åŠ¨çš„æ‰“å°ï¼ŒèŠ‚çº¦äº†ä½ çš„æ—¶é—´ï¼Œè¿›è€Œç›´è§‰ä¸Šæé«˜äº†ä½ çš„æ•ˆç”¨ã€‚

å¹¿ä¹‰ä¸ŠåŠ¨ä½œä¹‹é—´çš„ä¾èµ–å…³ç³»åˆ†ä¸ºæ¶ˆæå’Œç§¯æ



**Negative relationships**

- **Resource induced**. This is where there are insufficient resources to allow both actions to occur simultaneously. For example, if we're both trying to walk through the same door, or if two agents both need access to the same printer.

- **Incompatibility**. This is where the actions require mutually exclusive states. For example, you may need to sit an exam, which requires that your phone is switched off. And your partner may be booking you both a holiday, which requires your phone to be switched on so that they can contact you to confirm the dates.

**Positive relationships**

- **Requested**. This is where an agent explicitly asks for help with an action. For example, via the Contract Net Protocol.

- **Action equality**. This is where the agents recognise that they are both planning to perform the same action, and so one can perform it and save the other agent the work. For example, two students may realise that they are both planning to post the same question to the KEATS forum, and so agree that only one of them will do this. æˆ‘ä»¬éƒ½æ‰“ç®—æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œï¼Œå¹¶ä¸”ç›¸äº’çŸ¥é“ï¼Œè¿™æ ·çš„è¯æˆ‘ä»¬ä¸­çš„ä¸€ä¸ªèƒ½ç‹¬ç«‹å»å®Œæˆä»»åŠ¡ï¼Œä»è€ŒèŠ‚çœå¦ä¸€ä¸ªäººçš„åŠªåŠ›

- **Consequence**. This is where an action that one agent is planning has a side effect of achieving one of another agent's goals. For example, if I plan to drive the car I share with my partner tomorrow, I may need to first fill it up with petrol. My partner may have the goal that the car has some petrol in it, and so I am achieving their goal as a side effect of my action to drive the car. æˆ‘è®¡åˆ’ä¸­çš„åŠ¨ä½œçš„æ‰§è¡Œå¯ä»¥å¯¼è‡´ä½ çš„æŸä¸ªç›®æ ‡çš„å®Œæˆï¼Œè¿™æ˜¯æˆ‘åŠ¨ä½œçš„å‰¯ä½œç”¨ï¼Œè¿™æ ·å°±è§£é™¤äº†ä½ å»å®Œæˆè¯¥ç›®æ ‡çš„éœ€è¦

- **Favour**. This is similar to the consequence relationship. The action that one agent is planning has a side effect of contributing to another agent's goals, for example by achieving some precondition for an action in the other agent's plan. My flatmate may have a goal to cook a nice dinner, and the plan for this requires that milk is available. If I'm planning to do the weekly shopping, which involves buying milk, this helps my flatmate to achieve their goal. æˆ‘è®¡åˆ’ä¸­çš„éƒ¨åˆ†æœ‰è¿™æ ·çš„å‰¯ä½œç”¨ï¼Œå…¶æ‰§è¡Œä¼šå¯¹ä½ ç›®æ ‡çš„å®ç°äº§ç”Ÿè´¡çŒ®ï¼Œä¹Ÿè®¸æ˜¯ä½¿ç›®æ ‡çš„å®ç°å˜å¾—æ›´å®¹æ˜“ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡å®ç°ä½ åŠ¨ä½œçš„ä¸€ä¸ªå‰ææ¡ä»¶ï¼‰

æˆ‘ä»¬è®¾è®¡çš„ç³»ç»Ÿå¸Œæœ›åˆ©ç”¨ç§¯æä½œç”¨ï¼Œæ¶ˆé™¤æ¶ˆæä½œç”¨

One approach agents can use to coordinate their activities is **multi-agent planning**.

- **Centralised planning for distributed agents**

  In this approach, there is a coordinating agent who has all the information about each individual agent's private goals and available actions. The coordinating agent uses this information to find an effective global plan for all of the agents.

  

- **Partial global planning**

  Here, the agents plan for themselves but by sharing this information they also build up a (partial) view of the global plan of all agents, which they can use to improve their own plans. They then share their revised partial view of the global plan with the other agents, allowing them to improve their own plans, who then share their revised partial view of the global plan, and so on. 

- **Centralised merging of individual plans**

  In this approach, the agents first plan for themselves (according to the information they have about their own goals and actions) and then share their local plans with a coordinating agent. This coordinating agent then must merge these plans to create an efficient global plan for all the agents. 

### 5.3.1 Normative multi-agent systems

 In the multi-agent planning approach, we saw how this can be achieved by the agents sharing information about their actions and goals (either directly with each other, or with a centralised coordinating agent), but often agents may not want to share this private information with other agents in the system.

An agent can also have expectations about the behaviour of other agents because of knowledge it has about the system they are working in. An agent can expect certain behaviours because it is *normal* (i.e., a conventional way to act) or because it is *normative* (i.e., it is an expected standard set by the system). This is the area of **normative multi-agent systems**. 

Norms can be **prescriptive** (that is, standards set by the system, so normative). Some examples:

- In the UK, you must drive on the left (unless you are on a one way street or on private property).

- At King's, you must not plagiarise, cheat or collude on assessments.

Or norms can be **conventions** (that is, normal expected behaviours). Some examples:

- If it's your friend's birthday, you should buy them a present.

- People in the UK typically have dinner around 7pm.

- If you use an escalator in the UK, you should either stand on the right or walk up it on the left.

#### 5.3.1.1 Prescriptive Norms

*< target, deontic-component, context, content, punishment >*

where

*target* defines the agents to whom the norm applies;

*deontic-component* denotes the type of norm, either obligation, prohibition or permission;

*context* defines when the norm applies;

*content* is the behaviour that is constrained by the norm;

*punishment* is what happens if an agent gets caught violating the norm. 

- An **obligation** norm says that "in the given *context*, an agent of the *target* type is obliged to perform (i.e., they must perform) the *content* behaviour, and if they get caught not doing so they will receive the *punishment*."

â€‹		For example the norm 

â€‹		*< all-drivers, obligation, at-red-light, stop, 3-points-on-licence >*

â€‹		says that all drivers must stop when they're at a red light, and if they're caught not doing so they will get 3 points on their 		licence. 

- A **prohibition** norm says that "in the given *context*, an agent of the *target* type is prohibited from performing (i.e., they must not perform) the *content* behaviour, and if they get caught doing so they will receive the *punishment*." 

â€‹		For example the norm 

â€‹		*< all-drivers, prohibited, on-public-UK-road, drive-on-right, 6-points-on-licence >*

â€‹		says that all drivers must not drive on the right when they're on a public road in the UK, and if they're caught doing so they 		will get 6 points on their licence. 

- **Permission** norms are slightly different. These do not say that a *target* agent must or must not do something, but rather that it is permitted to do the *content* behaviour in the given *context*. Permission norms are typically used to identify exceptions to prohibitions and obligations, and as such do not normally have a *punishment* associated with them. 

â€‹		For example the norm 

â€‹		*< all-drivers, permitted, on-one-way-UK-street, drive-on-right, __ >*

â€‹		says that all drivers are permitted to drive on the right if they are on a one way street in the UK.

â€‹		Another example, the norm

â€‹		*< ambulance-drivers, permitted, at-red-light AND on-way-to-emergency, go, __ >*

â€‹		says that ambulance drivers are permitted to drive through red lights as long as they are on their way to an emergency.



An agent's reasoning about what to do is affected by its knowledge of the norms in the system:

An agent knows that if it is caught violating a prescriptive norm, this will lead to a punishment (i.e., some decrease in its utility).

If an agent violates a convention this will not cause direct punishment, but it may indirectly cause some decrease in utility by hindering the agent's coordination with other agents in the system.

So an agent can choose to violate a norm, but in doing so it should weigh up any potential negative consequences of this.

Consider the following prescriptive norm:

< *all-drivers,* *prohibited, on-UK-highway, drive-over-70-mph, fine-Â£50 >*

This says that all drivers on UK highways are prohibited from driving over 70 miles per hour, and if they get caught violating this norm they will get a Â£50 fine.



Let's look together now at some different reasons a driver on a UK highway might have for disobeying this norm:

< *all-drivers,* *prohibited, on-UK-highway, drive-over-70-mph, fine-Â£50 >*

**Ignorance**: the agent may not be aware of the norm. For example, if the driver doesn't normally drive in the UK and they haven't seen any of the road signs.

**Lack of understanding**: the agent may not understand what the norm means. For example, if the driver does not know what counts as a UK highway.

**Punishment is too low**: if the punishment is too low, then the agent has little incentive to obey the norm (there will be little impact on their utility if they get caught). For example, if the driver is a millionaire they may not care whether they get a Â£50 fine or not.

**Probability of getting caught is too low**: the agent may be willing to risk the punishment if they believe there is little chance that they will get caught. For example, if the driver believes they can out run any police.

**Another goal may be more important**: if obeying the norm will mean that the agent cannot achieve an important goal, the loss in utility from not achieving this goal may be greater than any punishment from disobeying the norm. For example, if the driver's partner is about to give birth and the driver is racing to get to the hospital to meet them, it may be so important to the driver that they are with their partner for the birth that they are willing to take the penalty if they get caught disobeying the norm.

**There may be no choice**: an agent may not have the capability to comply with the norm. For example, if the driver's accelerator pedal gets stuck!

These reasons hint at some of the challenges involved in designing appropriate prescriptive norms to govern a system:

**Agents need to know the norms.** There must be an efficient way of communicating the norms to all the agents in the system. This becomes more complicated if the system is open (meaning that that agents can freely choose whether to join or leave the system at any time), as it becomes harder to ensure that the agents that are currently part of the system are all aware of the norms.

**Agents need to understand the norms.** Think back to the discussions earlier on in this topic about agent communication, and how the agents need a shared understanding of the meaning of the terms that they exchange. The agents need a shared understanding of the norms in the system and of the terms that are used to define them.

**Monitoring of agents' adherence to the norms needs to be effective.** There is a balance to be struck here between the amount of effort involved in monitoring the agents' behaviours, and the likelihood that any agents disobeying a norm will be caught.

**Punishments need to be suitable.** Punishments need to be sufficient that they will dis-incentivise agents from disobeying the norms, but shouldn't be so great that they completely curtail the agents' autonomy. If the agents (or the agent designers) view the punishments as unreasonably severe, they may decide that the system does not allow them sufficient individual choice to disobey norms when they feel it is appropriate to do so.

The other key challenge is **how to design a suitable set of prescriptive norms that will encourage the types of behaviour that are desired from the system.** This is similar to the challenges faced by the designer of a reactive agent, where it can be very hard to come up with an appropriate set of individual behaviours that, when working together in a subsumption architecture, will produce the desired behaviour. It can be very hard to understand how all the different possible interactions between the norms and the different agents in the system will impact on the overall behaviour of the system.

While prescriptive norms are designed for a system, conventions *emerge* from the behaviours of the agents in the system. Typically, conventions emerge over time due to agents copying each others' behaviours. For example, imagine you visit a country you've not been to before, and when you go on an escalator for the first time you notice that everyone seems to be either standing on the left or walking on the right. You are likely to adopt the same behaviour, and so will be better coordinated with the other people using the escalator. 

That finishes our discussion of different mechanisms agents can use to help coordinate their behaviour. The last thing we will cover in this topic is trust and reputation.

### 5.4 Trust and reputation

è¯¥ä¿¡ä»»å“ªä¸ªagentå‘¢

There are two main ways an agent can determine how reliable it believes another agent to be: based on its own experience (trust) and based on the experience of others (reputation).

**Trust**. An agent can determine its measure of trust in another agent according to its past experience with that agent. if I've used two different delivery companies in the past, one of which consistently delivered later than promised and the other of which was always on time, I will trust the company which is always on time more than I trust the one that is always late.

**Reputation**. This is derived from the experience of other agents. Did the agent do a good job for others in the past? For example, if a company receives consistently negative reviews we would view that company as having a poor reputation. Or if agent Ag1 tells Ag3 about their bad experiences with Ag2, this may affect Ag3's view of Ag2's reputation. 

### 5.4.1 FIRE Model

how an agent can calculate its trust of another agentï¼Ÿ

The **FIRE model** is a model of trust and reputation that agents can use to help them decide who they might delegate tasks to



FIRE uses four different sources of information to judge another agent's reliability. Let us refer to the agent who is judging another agent's reliability as the *evaluator*, and the agent whose reliability is being judged as the *target* agent. The four sources of information that the evaluator can use are:

**Direct experience.** The evaluator can use the knowledge it has of its previous interactions with the target agent. Trust derived from direct experience is called **Interaction Trust**.

**Witness information.** The evaluator can use information about other agents' experiences of interacting with the target agent (these other agents are the *witnesses*). Trust derived from witness information is called **Witness Reputation**.

**Role-based rules.** This is where the evaluator can have certain expectations about the target agent because of some role that the target holds. For example, if the evaluator and the target agent are both part of the same organisation, the evaluator may be inclined to trust the target agent. Trust derived from role-based rules is called **Role-based Trust**.

**Third-party references provided by the target agent.** This is similar to witness information, except the target agent itself collects the references and can provide these to the evaluator. Trust derived from third-party references is called **Certified Reputation**.

E.g:

An agent, Ag0, determines who to delegate a task to using a simplified FIRE trust model, in which it uses only interaction trust to compare agents, only considers a single quality-of-service term, and uses a recency function such that a rating of an interaction on one day is given twice the weight of an interaction from the day before. Agents Ag1, Ag2 and Ag3 offer to perform the task.

- 10 days ago, Ag0 delegated to Ag1 with rating 0.5 and to agent Ag3 with rating 0.8.
- 8 days ago, Ag0 delegated to Ag1 with rating 0.9 and delegated to Ag2 with rating 0.6.
- 7 days ago, Ag0 delegated to Ag2 with rating 0.3.

Which of the following statements are true?

è§£æï¼š

**Ag0's trust in Ag1**

Ag0 has had two prior interactions with Ag1: 10 days ago with a rating of 0.5; and 8 days ago with a rating of 0.9. We need to find the weighted average of these two ratings, where the weights applied to the ratings are such that the rating of an interaction on one day is given twice the weight of an interaction from the day before. Since the interaction rated 0.5 came two days before the interaction rated 0.9, we need to weight the 0.9 rating at 4 times the 0.5 rating. (If the rating of an interaction on one day is given twice the weight of an interaction from the day before, then the rating of an interaction must be given four times the weight of an interaction from two days ago.) Ag0's trust in Ag1 is calculated as the weighted average of the ratings (i.e., the sum of the weighted ratings divided by the sum of the weights): 

((1 x 0.5) + (4 x 0.9))/(1+4) = 4.1/5 = 0.82   ï¼ˆæœ€è¿‘ä¸€å¤©æ˜¯å‰ä¸€å¤©æƒé‡çš„ä¸¤å€ï¼‰

**Ag0's trust in Ag2:**

Ag0 has had two prior interactions with Ag2: 8 days ago with a rating of 0.6; and 7 days ago with a rating of 0.3. We need to find the weighted average of these two ratings, where the weights applied to the ratings are such that the rating of an interaction on one day is given twice the weight of an interaction from the day before. Since the interaction rated 0.6 came one day before the interaction rated 0.3, we need to weight the 0.3 rating at 2 times the 0.6 rating. Ag0's trust in Ag2 is calculated as the weighted average of the ratings (i.e., the sum of the weighted ratings divided by the sum of the weights): 

((1 x 0.6) + (2 x 0.3))/(1+2) = 1.2/3 = 0.4

**Ag0's trust in Ag3:**

Ag0 has only had one prior interaction with Ag3: 10 days ago with a rating of 0.8. As there is only one interaction to consider, the weighted average of its rating is straightforward, it's simply equal to the rating. So Ag0's trust in Ag3 is 0.8.

Since Ag0 trusts Ag1 more than it does either Ag2 or Ag3, it selects Ag1 to delegate the task to.





# Wee6: Game theory, and negotiation

## 6.1 Game Theory

### 6.1.1 Dominant strategy 

ä¼˜åŠ¿ç­–ç•¥ï¼ša strategy $s_i$ is dominant for agent $ğ‘–$ if, no matter what the other agents do, agent ğ‘– can't do any better than play $s_i$.

A **rational** agent will play a dominant strategy(if one exists).

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211229210859331.png" alt="image-20211229210859331" style="zoom:30%;" />

Is there a dominant strategy for player 1?? **Steal**

å¦‚æœP2é€‰æ‹©Splitï¼Œé‚£å¯¹äºP1è€Œè¨€æœ€å¥½çš„é€‰æ‹©å°±æ˜¯Stealï¼Œå› ä¸ºèƒ½è·å¾—10kï¼›å¦‚æœP2é€‰æ‹©äº†Stealï¼Œå¯¹äºP1è€Œè¨€é€‰Splitå’ŒStealéƒ½ä¸€æ ·ï¼Œå› ä¸ºéƒ½åªèƒ½è·å¾—0ï¼›ç»¼ä¸Šæ‰€è¿°ï¼Œdominant strategy for P1 is **Steal**

E.g: 

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211229211224774.png" alt="image-20211229211224774" style="zoom:40%;" />

### 6.1.2 Dominanted strategy

åŠ£åŠ¿ç­–ç•¥ï¼Œæˆ‘ä»¬å¸Œæœ›åˆ é™¤åŠ£åŠ¿ç­–ç•¥

A rational agent will never play a dominated strategy

### 6.1.3 Nash Equilibrium

åŒæ–¹éƒ½ä¸èƒ½æ”¹å˜ç­–ç•¥ä»ä¸­è·ç›Š(better off)

P1å’ŒP2éƒ½é€‰æ‹©Stealï¼Œæ˜¯Nash Equilibriumã€‚å› ä¸ºå‡è®¾P2ä¸æ”¹å˜é€‰æ‹©ï¼ŒP1é€‰æ‹©äº†Splitï¼Œä¾ç„¶è¿˜æ˜¯è·ç›Š0ï¼›å‡è®¾P1ä¸æ”¹å˜é€‰æ‹©ï¼ŒP2é€‰æ‹©äº†Splitï¼Œä¾ç„¶è·ç›Šæ˜¯0ï¼Œå› æ­¤æ˜¯Nash Equilibriumã€‚

P1é€‰æ‹©Splitï¼ŒP2é€‰æ‹©Stealï¼Œæ˜¯Nash Equilibriumã€‚å› ä¸ºå‡è®¾P2ä¸æ”¹å˜é€‰æ‹©ï¼ŒP1é€‰æ‹©äº†Stealï¼Œä¾ç„¶è¿˜æ˜¯è·ç›Š0ï¼›å‡è®¾P1ä¸æ”¹å˜é€‰æ‹©ï¼ŒP2é€‰æ‹©äº†Splitï¼Œè·ç›Šå‡å°‘åˆ°5Kï¼Œæ²¡æœ‰ä»ä¸­è·ç›Šï¼Œå› æ­¤æ˜¯Nash Equilibriumã€‚



æŠ€å·§ï¼šæ¨ªç€æ‰¾å‡ºæ¯ä¸€è¡Œç¬¬äºŒä¸ªæ•°å­—æœ€å¤§çš„ï¼Œåœ¨åº•ä¸‹åˆ’æ¡çº¿ï¼›ç«–ç€æ‰¾å‡ºæ¯ä¸€åˆ—ç¬¬ä¸€ä¸ªæ•°å­—æœ€å¤§çš„ï¼Œåœ¨åº•ä¸‹ç”»ä¸€æ¡çº¿ï¼Œæœ€åæ‰¾å‡ºæœ‰ä¸¤æ¡çº¿çš„ç»„åˆå°±æ˜¯çº³ä»€å‡è¡¡ã€‚

### 6.1.4 Pareto Optimal

å¦‚æœ**ä¸å­˜åœ¨**å…¶ä»–çš„åå•†ç»“å±€ä½¿è‡³å°‘ä¸€ä¸ªAgent æ›´å¥½è€Œæ²¡æœ‰ä½¿å…¶ä»– Agent æ›´å·®ï¼Œåˆ™ç§°è¿™ä¸ªåå•†ç»“å±€å…·æœ‰Pareto æ•ˆç‡ã€‚æ‰€ä»¥å¦‚æœåå•†ç»“å±€æ²¡æœ‰è¾¾åˆ°Pareto æ•ˆç‡åˆ™å­˜åœ¨å¦ä¸€ä¸ªåå•†ç»“å±€ä½¿å¾—è‡³å°‘æœ‰ä¸€ä¸ªAgent æ›´æ»¡æ„ï¼ŒåŒæ—¶ä½¿å…¶ä»–æ‰€æœ‰çš„Agent è‡³å°‘æ»¡æ„

1. è¾¾åˆ°å¦‚æœå†æ”¹å˜å°±ä¼šä½¿ä¸€æ–¹å—ç›Šï¼Œå¦ä¸€æ–¹å—æŸçš„æœ€ä¼˜ä¸´ç•Œç‚¹
2. ä¸¤è€…åŒæ—¶è¾¾åˆ°æœ€å¥½ï¼Œæ²¡æœ‰æ¯”è¿™ä¸ªçŠ¶æ€æ›´å¥½çš„

P1 Stealï¼ŒP2 Stealå°±ä¸æ˜¯Pareto Optimalï¼Œå› ä¸ºP2å¯ä»¥é€‰æ‹©Splitæ¥æ‹¿åˆ°10Kçš„æ”¶ç›Šï¼Œä½†åŒæ—¶P1çš„æ”¶ç›Šæ²¡å—åˆ°æŸå®³



### 6.1.5 Maximising social welfare

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211229220849344.png" alt="image-20211229220849344" style="zoom:50%;" />

## 6.2 Negotiation

#### 6.2.1 Task Oriented Domains (TOD)

é¢å‘ä»»åŠ¡é¢†åŸŸåå•†

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211229224124696.png" alt="image-20211229224124696" style="zoom:30%;" />

åœ¨Initial encounterä¸­ï¼ŒAgent1åˆå§‹è¢«åˆ†é…çš„æ˜¯shop, cook, washUp; Agent2åˆå§‹è¢«åˆ†é…çš„æ˜¯getVideo,tidy,buyWine

å…¶ä¸­Agent1è¢«åˆ†é…ä»»åŠ¡çš„costæ˜¯12ï¼ŒåŒç†Agent2æ˜¯10ï¼›

åœ¨æ–°åˆ†é…çš„ä»»åŠ¡ä¸­ï¼ŒAgent1ä¸å†éœ€è¦åšshopè€Œéœ€è¦åštidyï¼Œæ–°dealæ˜¯cook, tidy,washUp;åŒç†ï¼ŒAgent2çš„æ–°dealæ˜¯shopï¼ŒgetVideo,buyWineã€‚

æ‰€ä»¥Agent1çš„Utilityæ˜¯12-9=3ï¼›Agent2çš„æ˜¯10-6=4ï¼›

å› æ­¤Agent1åšæ–°çš„dealå¯ä»¥èŠ‚çœ3çš„costï¼ŒAgent2åšæ–°çš„dealå¯ä»¥èŠ‚çœ4çš„costï¼›æ–°dealæ¯”åˆè¯•dealè¦æ›´åˆé€‚ã€‚

```
Utility = a  - b

a = the cost of what was original allocated to the agents in the initial encounter
b = the cost of agent has been allocated in the new deal

ä¸€é¡¹äº¤æ˜“çš„Utilityä»£è¡¨Agentä»è¯¥äº¤æ˜“ä¸­è·å¾—æ”¶ç›Šçš„å¤šå°‘
å¦‚æœUtilityæ˜¯è´Ÿæ•°ï¼Œåˆ™è¯æ˜ä»£ç†æ‰§è¡Œæœ€åˆåˆ†é…çš„ä»»åŠ¡æ¯”æ‰§è¡Œäº¤æ˜“ä¸­åˆ†é…çš„ä»»åŠ¡æ›´å¥½; å¦‚æœæ˜¯æ­£æ•°ï¼Œåˆ™è¯æ˜æ‰§è¡Œæ–°ä»»åŠ¡æ¯”è¾ƒå¥½
```

- conflict deal: If agreement is not reached then we say that the result is the **conflict deal**. It is assumed that the conflict deal is the worst outcomeã€‚**å¯¹äºconflict dealçš„utilityæ˜¯è´Ÿçš„**
- **Individual rational**: A deal Î´ is said to be **individual rational** if neither agent prefers the conflict deal to Î´.
- **negotiation set**ï¼šç”±äº¤æ˜“ç»„æˆï¼Œè¿™äº›äº¤æ˜“(å…·æœ‰ä¸ªä½“ç†æ€§ï¼Œå¹¶ä¸”(i)æ˜¯pareto æœ€ä¼˜çš„ã€‚ç¬¬ä¸€ä¸ªçº¦æŸèƒŒåçš„ç›´è§‚å«ä¹‰æ˜¯æŒ‡,æå‡ºå¯¹æŸä¸€ Agentæ¥è¯´æ¯”å†²çªäº¤æ˜“æ›´ä¸å¥½çš„äº¤æ˜“æ˜¯æ²¡æœ‰æ„ä¹‰çš„(å› ä¸ºè¿™æ ·çš„è¯ Agent å®æ„¿å†²çª)ã€‚ç¬¬äºŒä¸ªæ¡ä»¶èƒŒåçš„ç›´è§‚å«ä¹‰æ˜¯, æå‡ºä¸€ä¸ªå»ºè®®æ—¶ï¼Œå¦‚æœè¿˜æœ‰å¦å¤–çš„å»ºè®®å¯¹æŸä¸€ Agent æ›´å¥½è€Œæ²¡æœ‰å…¶ä»–Agent å—åˆ°æŸå¤±ï¼Œåˆ™è¿™ä¸ªå»ºè®®æ˜¯æ²¡æœ‰æ„ä¹‰çš„.

### 6.2.2 Monotonic Concession Protoco

- åå•†è¿›è¡Œå¤šè½®
- åœ¨ç¬¬ä¸€è½®åå•†ä¸­ï¼Œä¸¤ä¸ªagentåŒæ—¶ä»negotiation setä¸­æå‡ºä¸€é¡¹äº¤æ˜“
- å¦‚æœä¸¤ä¸ªAgentæå‡ºçš„äº¤æ˜“åˆ†åˆ«æ˜¯$\delta_1$å’Œ$\delta_2$,ä½¿å¾—æˆ–è€…æœ‰$utility_1(\delta_2) >= utility_1(\delta_1)$ï¼Œæˆ–è€…$utility_1(\delta_1) >= utility_1(\delta_2)$ï¼Œåˆ™è¾¾æˆä¸€è‡´ã€‚**å³å¦‚æœæœ‰ä¸€ä¸ªAgentå‘ç°å¦ä¸€ä¸ªagentæå‡ºçš„äº¤æ˜“è‡³å°‘ä¸è‡ªå·±æå‡ºçš„ä¸€æ ·å¥½æˆ–è€…æ¯”è‡ªå·±æå‡ºçš„æ›´å¥½ã€‚**
- å¦‚æœä¸èƒ½è¾¾æˆä¸€è‡´ï¼Œåˆ™åå•†ç»§ç»­è¿›è¡Œå¦ä¸€è½®ï¼ŒåŒæ—¶æå‡ºå»ºè®®ã€‚åœ¨ç¬¬u+1è½®ï¼Œä¸å…è®¸Agentæå‡ºæ¯”ç¬¬uè®ºå»ºè®®å¯¹å…¶ä»–Agentæ›´å·®çš„å»ºè®®
- å¦‚æœåœ¨æŸä¸€è½®u>0ï¼Œæ²¡æœ‰agentåšå‡ºè®©æ­¥ï¼Œåˆ™åå•†ä»¥conflict dealç»“æŸ



### 6.2.3 Zeuthen strategy

å†³å®šäº†å‚ä¸è€…åœ¨ä½¿ç”¨Monotonic Concession Protocoæ˜¯åº”è¯¥å¦‚ä½•å·¥ä½œ

- Agentçš„ç¬¬ä¸€ä¸ªå»ºè®®åº”è¯¥æ˜¯ä»€ä¹ˆï¼Ÿ åº”è¯¥æ˜¯Agentçš„ç¬¬ä¸€ä¸ªå»ºè®®åº”è¯¥æ˜¯ä»–æœ€å¸Œæœ›çš„äº¤æ˜“
- åœ¨ç»™å®šçš„ä¸€è½®åè®®ä¸­ï¼Œè°åº”è¯¥è®©æ­¥ï¼Ÿ
- å¦‚æœä¸€ä¸ªAgentè®©æ­¥ï¼Œä»–åº”è¯¥è®©æ­¥å¤šå°‘ï¼Ÿ



åœ¨ç»™å®šçš„ä¸€è½®åè®®ä¸­ï¼Œè°åº”è¯¥è®©æ­¥ï¼Ÿ

åº¦é‡ Agentå¯¹å†²çªé£é™©çš„æ„æ„¿ã€‚ç›´è§‚ä¸Šï¼Œå¦‚æœAgent å½“å‰å»ºè®®çš„æ•ˆç”¨ä¸å†²çªäº¤æ˜“çš„æ•ˆç”¨å·®åˆ«å°ï¼Œåˆ™å®ƒæ›´æ„¿æ„éª¨å†²çªçš„é£é™©ã€‚ç›¸åï¼Œå¦‚æœä¸€ä¸ªAgent å½“å‰çš„å»ºè®®ä¸å†²çªäº¤æ˜“çš„å·®åˆ«å¤§ï¼Œåˆ™å†²çªæ—¶ï¼Œè¿™ä¸ªAgentä¼šé­å—æ›´å¤§çš„æŸå¤±ï¼Œå› æ­¤å®ƒæ›´ä¸æ„¿æ„èƒƒå†²çªé£é™©--æ›´æ„¿æ„è®©æ­¥ã€‚

Let's assume that $Î´i$ is the deal currently proposed by $Agi$ and $ğ›¿ğ‘—$ is the deal currently proposed by $Agj$. Formally, $Agi's$ willingness to risk the conflict deal is equal to

$risk = \frac{utility_i(\delta_i) - utility_i(\delta_j)}{utility_i(\delta_i)}$

åˆ†å­å®šä¹‰ä¸ºiå½“å‰å»ºè®®çš„æ•ˆç”¨ä¸jçš„å»ºè®®å¯¹äºiçš„æ•ˆç”¨çš„å·®;åˆ†æ¯å®šä¹‰ä¸º Agent iå½“å‰å»ºè®®çš„æ•ˆç”¨ã€‚åœ¨è¾¾æˆä¸€è‡´ä»¥å‰ã€‚risk çš„å€¼åœ¨0~1ä¹‹åŒã€‚risk çš„å€¼è¶…å¤§ï¼ˆè¶Šæ¥è¿‘1ï¼‰ï¼Œè¡¨ç¤ºiç”±å†²äº¤é­å—çš„æŸå¤±è¶Šå°ï¼Œå› æ­¤æ›´æ„¿æ„å†’å†²çªé£é™©ã€‚åä¹‹ï¼Œriskçš„å€¼å°ï¼ˆè¶Šæ¥è¿‘0ï¼‰ï¼Œè¡¨ç¤ºç”±å†²çªé£å—çš„æŸå¤±è¶Šå¤§ï¼Œå› æ­¤æ›´ä¸æ„¿æ„å†’å†²çªé£é™©ã€‚





# Week7: Making Group Decision

## 7.1 Plurality Voting Protocol

**Only takes into account the top preference of each voter**

E.g: 

Î©={ğœ”1,ğœ”2,ğœ”3}

40 agents have the preference ğœ”1â‰»ğœ”2â‰»ğœ”3

30 agents have the preference ğœ”2â‰»ğœ”3â‰»ğœ”1

30 agents have the preference ğœ”3â‰»ğœ”2â‰»ğœ”1

Under the plurality protocol:

- Ï‰1 gets 40 points.
- Ï‰2 gets 30 points.
- Ï‰3 gets 30 points.



## 7.2 Condorcet Winner

å¦‚æœåœ¨æˆå¯¹çš„å¤šæ•°ç«èµ›ä¸­ï¼Œæœ‰ä¸€ä¸ªç»“æœå‡»è´¥äº†æ‰€æœ‰å…¶ä»–ç»“æœï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯å­”å¤šå¡èµ¢å®¶

Î©={ğœ”1,ğœ”2,ğœ”3}

40 agents have the preference ğœ”1â‰»ğœ”2â‰»ğœ”3

30 agents have the preference ğœ”2â‰»ğœ”3â‰»ğœ”1

30 agents have the preference ğœ”3â‰»ğœ”2â‰»ğœ”1

Pairwise majority contests

ğœ”1 vs ğœ”2: 40 prefer ğœ”1, 60 prefer ğœ”2, ğœ”2 wins

ğœ”1 vs ğœ”3: 40 prefer ğœ”1, 60 prefer ğœ”3, ğœ”3 wins

ğœ”2 vs ğœ”3: 70 prefer ğœ”2, 30 prefer ğœ”3, ğœ”2 wins

So here, ğœ”2 is the Condorcet winner, as it beats both ğœ”1 and ğœ”3 in pairwise majority contests, but under plurality voting, ğœ”1wins.



## 7.3 Linear sequential majority elections

- There are a series of rounds.
- A pair of outcomes face each other in a pairwise majority election, the winner goes on to the next round.
- The order in which the outcomes face each other is determined by an agenda.

**Example:** If the agenda is ğœ”4, ğœ”2, ğœ”3, ğœ”1 then the first election is between ğœ”4 and ğœ”2, the winner of that goes onto an election with ğœ”3, and the winner of that goes onto an election with ğœ”1. 

E.g1:

Let Î©={ğ´,ğµ,ğ¶}. Consider the following voter profile.

3 voters have the preference ğ´â‰»ğµâ‰»ğ¶

2 voters have the preference ğµâ‰»ğ¶â‰»ğ´

1 voter has the preference ğµâ‰»ğ´â‰»ğ¶

1 voter has the preference ğ¶â‰»ğ´â‰»ğµ

The vote is carried out under the linear sequential majority election protocol with the agenda ğ´,ğ¶,ğµ. Which candidate is selected as the winner? 

è§£æ

First round: *A* vs *C*. 4 voters prefer A to C; 3 voters prefer C to A. A goes through to the next round. 

Second round: *A* vs *B*. 4 voters prefer A to B. 3 voters prefer B to A. A wins. 



E.g2:

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211230014439592.png" alt="image-20211230014439592" style="zoom:45%;" />

- It is possible to fix the agenda so that A wins under the linear sequential election voting protocol.
  - å› ä¸ºC, D, B, A (notice there is a path from A to B to D to C)ï¼›
  - æˆ–D, B, C, A (notice there is a path from A to C to B to D)
- It is possible to fix the agenda so that B wins under the linear sequential election voting protocol.
  - C, A, D, B (notice there is a path from B to D to A to C)
- It is possible to fix the agenda so that C wins under the linear sequential election voting protocol.
  - A, D, B, C (notice there is a path from C to B to D to A)
- It is possible to fix the agenda so that D wins under the linear sequential election voting protocol.
  - B, A, C, D (D->A->C->B)



## 7.4 Instant Runoff Voting Protocol

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211230015617699.png" alt="image-20211230015617699" style="zoom:40%;" />

## 7.4 Copeland rule voting protocol

Under the Copeland rule, all candidates face each other in pairwise majority elections. Each candidate receives:

- 1 point for every pairwise majority election that they win;
- -1 point for every pairwise majority election that they lose;
- 0 points for every pairwise majority election they draw at.

The candidate with the most points is the winner.

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211230020815234.png" alt="image-20211230020815234" style="zoom:40%;" />

## 7.5 Borda count voting protocol

Under the Borda count protocol, where ğ‘š is the number of candidates, each candidate receives:

- (ğ‘šâˆ’1)points for each voter that lists them as their first choice;
- (ğ‘šâˆ’2) points for each voter that lists them as their second choice;
- and so on.

The candidate with the most points is the winner.

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211230021118136.png" alt="image-20211230021118136" style="zoom:40%;" />



### 7.3 Strategy-proofness

é˜²ç­–ç•¥(Strategy-proofness)é€šå¸¸è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªéå¸¸ç†æƒ³çš„å±æ€§,å®ƒè¦æ±‚æŠ•ç¥¨è€…ä¸èƒ½ä»è°æŠ¥ä»–ä»¬çš„çœŸå®åå¥½ä¸­è·ç›Š,è¿›è€Œå¯ä»¥æŠ‘åˆ¶ç¤¾ä¼šé€‰æ‹©ä¸­çš„ç­–ç•¥æŠ•ç¥¨,ä¿ƒä½¿æŠ•ç¥¨è€…éƒ½æŠ•å‡ºè‡ªå·±çš„çœŸå®é€‰ç¥¨,ä½¿é€‰ä¸¾ç»“æœèƒ½å¤Ÿä½“ç°äººä»¬çš„çœŸå®æ„æ„¿.

 A voting protocol is said to be strategy proof if and only if it is strategy proof for all voters. In other words no one has any incentive to misrepresent their preferences.

- **Dictatorships**: ç‹¬è£æ˜¯æŒ‡æ•´ä¸ªç¤¾ä¼šçš„æ€»ä½“åå¥½æ’åºä¼šåªç”±ä¸€ä¸ªäººçš„åå¥½æ’åºå†³å®šï¼Œè¿™ä¸ªäººæœ‰å¯èƒ½æ˜¯éšè—åœ¨å¤æ‚çš„æŠ•ç¥¨æœºåˆ¶ä¸‹çš„ï¼Œä¸ä¸€å®šæ˜¯ç­é•¿é‚£ç§æ˜¾è€Œæ˜“è§çš„ç‹¬è£è€…ã€‚If a voting protocol is a dictatorship then it ignores the preferences of all voters except for those of the dictator *i*. **ä¸å­˜åœ¨è¿™æ ·çš„é€‰æ°‘ï¼Œä½¿å¾—ä»–çš„é€‰æ‹©ä¸€å®šä¸ºæœ€åç»“æœã€‚*å­˜åœ¨æŸä¸ªé€‰ä¸¾äººï¼Œåªè¦ä»–è®¤ä¸ºä»»æ„ä¸¤ä¸ªå€™é€‰äººçš„å…³ç³»æ˜¯ A>B ï¼Œé‚£ä¹ˆå³ä½¿å…¶å®ƒé€‰ä¸¾äººéƒ½è®¤ä¸º A<B ï¼Œé€‰ä¸¾ç»“æœä¹Ÿä¸€å®šæ˜¯ A>B** ã€‚ 

- **Surjective**: for every candidate there is some profile of voter preferences such that the candidate will win.

  To put this another way, every candidate has a chance of winning. 

  This is a desirable property. Imagine we have a voting protocol that is not surjective, this would mean that there is some candidate which could not win -- even if every voter placed this candidate as their top choice.

- **Resolute**: there is always a unique winner (i.e., no ties).

  This is often a desirable property, since applications often require a unique winner.

  Note, the voting protocols we have looked at here can easily be made resolute by adding the extra condition that in the case of a tie a winner will be selected at random.

  

## 7.4 Gibbard-Satterthwaite Theorem

If we have 3 or more candidates, any **resolute** voting protocol that is **surjective** and **strategy proof** is a **dictatorship**.

**It says that it is not possible to design a voting protocol that is resolute, surjective, strategy proof and not a dictatorship.**The voting protocols we have looked at are resolute (assuming ties are resolved randomly), surjective, and not dictatorships, therefore they cannot be strategy proof.



## 7.5 Weekly Pareto

A voting protocol is said to be weakly Pareto if and only if:

- if there are candidates ğœ”ğ‘– and ğœ”ğ‘— such that **all** voters prefer ğœ”ğ‘–Ï‰i to ğœ”ğ‘—, then ğœ”j will not be selected as the winner.

In other words, if every voter prefers ğœ”ğ‘– to ğœ”ğ‘—, the outcome will also reflect this. Hopefully it is clear that this is a desirable property.å¦‚æœï¼Œé€‰ä¸¾ä¸­çš„æ¯ä¸€ä¸ªä»£ç†äººæ¯”è¾ƒAå’ŒBæ—¶éƒ½æ›´å–œæ¬¢Aï¼Œé‚£ä¹ˆBä¸å¯èƒ½æ˜¯æœ€ç»ˆçš„é€‰ä¸¾ç»“æœ

 è®¾â€œæ¬¢å–œâ€è¡¨ç¤ºâ€œï¼ˆæ¯”åŸæ¥ï¼‰å¤„å¢ƒä¸¥æ ¼å˜å¥½â€ï¼Œâ€œå¿§æ„â€è¡¨ç¤ºâ€œï¼ˆæ¯”åŸæ¥ï¼‰å¤„å¢ƒä¸¥æ ¼å˜å·®â€ã€‚ èµ„æºé…ç½®å˜åŒ–åï¼Œâ€œæœ‰äººæ¬¢å–œï¼Œæ— äººå¿§æ„â€ç›¸å¯¹äºâ€œæ— äººå¿§æ„â€æ˜¯â€œæ›´å¼ºçš„â€å¸•ç´¯æ‰˜æ”¹å–„ï¼Œç›¸å¯¹äºâ€œæ‰€æœ‰äººæ¬¢å–œâ€æ˜¯â€œæ›´å¼±çš„â€å¸•ç´¯æ‰˜æ”¹å–„ã€‚ è‹¥æ— è®ºèµ„æºé…ç½®å¦‚ä½•å˜åŒ–ï¼Œéƒ½ä¸å¯èƒ½â€œæœ‰äººæ¬¢å–œï¼Œæ— äººå¿§æ„â€ï¼Œåˆ™è¯¥çŠ¶æ€æ˜¯â€œå¼ºå¸•ç´¯æ‰˜æœ€ä¼˜â€ã€‚ è‹¥æ— è®ºèµ„æºé…ç½®å¦‚ä½•å˜åŒ–ï¼Œéƒ½ä¸å¯èƒ½â€œæ‰€æœ‰äººæ¬¢å–œâ€ï¼Œåˆ™è¯¥çŠ¶æ€æ˜¯â€œå¼±å¸•ç´¯æ‰˜æœ€ä¼˜â€ã€‚

## 7.6 Arrow's Theorem

å¸•ç´¯æ‰˜æœ€ä¼˜ï¼ˆParetoï¼‰
å¯¹äºå…¨éƒ¨çš„Nä¸ªæ’åˆ—é¡ºåºï¼Œå¦‚æœAéƒ½åœ¨Bä¹‹å‰ï¼Œé‚£ä¹ˆæœ€åçš„ç»“æœAä¸€å®šåœ¨Bä¹‹å‰ã€‚

æ— å…³å› ç´ ç‹¬ç«‹æ€§ï¼ˆIndependence of Irrelevant Alternatives (IIA)ï¼‰
ä¸¤ä¸ªäººçš„ç›¸å…³é¡ºåºä¸å˜çš„è¯ï¼Œå…¶ä»–å‚ä¸è€…çš„ç›¸å¯¹ä½ç½®å‘ç”Ÿäº†å˜åŒ–ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªäººçš„ç›¸å¯¹ä½ç½®ä¹Ÿä¸ä¼šå‘ç”Ÿå˜åŒ–ã€‚ æŠ•ç¥¨é—®é¢˜ï¼ˆvoting theoryï¼‰ä¸­ï¼Œå‡å¦‚æœ‰å››ä¸ªå€™é€‰äººï¼Œå³Aã€Bã€Cå’ŒDï¼Œå¦‚æœå¤§å¤šæ•°æ°‘ä¼—ï¼ˆå³è¶…è¿‡ä¸€åŠï¼‰ä¸€è‡´è®¤ä¸ºAä¼˜äºCï¼Œé‚£ä¹ˆBå’ŒDçš„ç›¸å¯¹ä½ç½®å‘ç”Ÿäº†å˜åŒ–ï¼Œä¹Ÿä¸ä¼šå½±å“å¤§å¤šæ•°æ°‘ä¼—çš„åå¥½ä¸ŠAä¼˜äºCã€‚

éç‹¬è£ï¼ˆNondictatorshipï¼‰
ä¸å­˜åœ¨è¿™æ ·çš„é€‰æ°‘ï¼Œä½¿å¾—ä»–çš„é€‰æ‹©ä¸€å®šä¸ºæœ€åç»“æœã€‚

**æ²¡æœ‰ä»»ä½•ä¸€ç§é€‰ä¸¾æ–¹å¼å¯ä»¥åŒæ—¶æ»¡è¶³ä»¥ä¸Šä¸‰ä¸ªæ¡ä»¶ã€‚å¯¹äºå…·æœ‰ä¸¤åä»¥ä¸Šå€™é€‰äººçš„é€‰ä¸¾ï¼Œä»»ä½•æ»¡è¶³å¸•ç´¯æ‰˜æ•ˆç‡å’ŒIIAè¦æ±‚çš„ç¤¾ä¼šç¦åˆ©èŒèƒ½éƒ½æ˜¯ç‹¬è£çš„â€”â€”Arrowâ€™så…¬ç†**  It says that it is not possible to design a voting protocol that is weakly Pareto, independent of irrelevant alternatives and not a dictatorship.

We've now seen two impossibility results (Arrow's Theorem and the Gibbard-Satterthwaite Theorem) that tell us it is impossible to design democratic (non-dictatorship) voting protocols that have intuitively desirable properties.



## 7.7 Single Peak Preferences and Median voter rule

Gibbard-Satterthwaiteå®šç†å’ŒArrow's Theoreméƒ½å‡è®¾é€‰æ°‘å¯ä»¥ç»™äºˆä»–ä»¬å–œæ¬¢çš„å€™é€‰äººçš„ä»»ä½•åå¥½ï¼ˆå³ï¼Œä»–ä»¬å¯ä»¥ä»¥ä»»ä½•é¡ºåºåˆ—å‡ºå®ƒä»¬ï¼‰ã€‚å¦‚æœæˆ‘ä»¬é™åˆ¶å…è®¸é€‰æ°‘æå‡ºçš„åå¥½å‘½ä»¤ï¼Œé‚£ä¹ˆè¿™äº›è´Ÿé¢ç»“æœå°±ä¸æˆç«‹ã€‚

**å•å³°åå¥½ç†è®º(Single Peak Preferences)**æ˜¯é™å®šæ¯ä¸ªé€‰æ°‘çš„åå¥½åªèƒ½æœ‰ä¸€ä¸ªå³°å€¼ã€‚

æ‰€è°“å•å³°åå¥½ï¼Œæ˜¯æŒ‡é€‰æ°‘åœ¨ä¸€ç»„æŒ‰æŸç§æ ‡å‡†æ’åˆ—çš„å¤‡é€‰æ–¹æ¡ˆä¸­ï¼Œæœ‰ä¸€ä¸ªæœ€ä¸ºåå¥½çš„é€‰æ‹©ï¼Œè€Œä»è¿™ä¸ªæ–¹æ¡ˆå‘ä»»ä½•æ–¹é¢çš„åç¦»ï¼Œé€‰æ°‘çš„åå¥½ç¨‹åº¦æˆ–æ•ˆç”¨éƒ½æ˜¯é€’å‡çš„ã€‚



å¦‚æœåœ¨ä¸€ä¸ªå¤šæ•°å†³ç­–çš„æ¨¡å‹ä¸­ï¼Œä¸ªäººåå¥½éƒ½æ˜¯å•å³°çš„ï¼Œåˆ™åæ˜ ä¸­é—´æŠ•ç¥¨äººæ„æ„¿çš„é‚£ç§æ”¿ç­–ä¼šæœ€ç»ˆè·èƒœï¼Œå› ä¸ºé€‰æ‹©è¯¥æ”¿ç­–ä¼šä½¿ä¸€ä¸ªå›¢ä½“çš„ç¦åˆ©æŸå¤±æœ€å°ã€‚





# Week8: Allocating Scarce Resources

## 8.1 English Auction

ä¼˜åŠ¿ç­–ç•¥ï¼ˆdominate strategyï¼‰ï¼šAgentç›¸ç»§ä»¥ç•¥é«˜äºå½“å‰å«ä»·çš„ä»·æ ¼å«ä»·ï¼Œç›´åˆ°å«ä»·åˆ°è¾¾ä»–ä»¬å½“å‰çš„ä¼°ä»·ã€‚bid the smallest amount possible above the current bid until the current bid price reaches your true valuation of the item being auctioned

E.g:

Consider an English auction with two bidders, ğ‘ğ‘”1ag1 and ğ‘ğ‘”2ag2, who are each playing their dominant strategy. 

We have 

- v1 = 7.5
- v2 = 300

where vi is agent agi's true valuation of the item. 

The minimum bid increment allowed is 1, and the auctioneer starts the bidding at 5.

How much will the auctioneer receive? (In other words, how much will the winning bidder end up paying for the item?) Assume that ag1 is always faster to bid than ag2 (that is, if both agents are willing to make a bid of b, ag1 will always be the agent who bids b).

Answer: 8

ä¸€æ—¦åˆ°äº†7ï¼ŒAg1å°±ä¼šåœæ­¢æ‹å–ï¼Œå› ä¸ºä¸‹ä¸€æ¬¡æ‹ä»·å°±æ˜¯8äº†è¶…è¿‡äº†Ag1çš„é¢„æœŸ7.5ï¼›ç”±äºè¿˜æ²¡è¾¾åˆ°Ag2çš„æœŸæœ›ï¼Œæ‰€ä»¥Ag2å‡ºä»·8ï¼Œæ— äººç«ä»·ï¼Œæœ€ç»ˆä»¥8ç»“æŸã€‚

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211231001643159.png" alt="image-20211231001643159" style="zoom:40%;" />

winner's valuation=300; 2nd highest valuation=7.5; 

### 8.1.1 Shill bids

è™šå‡å‡ºä»·ï¼šA shill bid is a bid placed by the auctioneer, or by agents in collusion with the auctioneer, with the aim of increasing the price paid by the winner. 

## 8.2 Dutch Auction

é€’å‡æ‹å–

### 8.2.1 Risk Averse

ä¸æ„¿æ„é•¿æ—¶é—´ç­‰å¾…ï¼Œå‡å°‘é£é™©ï¼Œå‡å°‘æ”¶ç›Š

Risk Averseä¸æ„¿æ„é•¿æ—¶é—´çš„ç­‰å¾…ï¼Œå› æ­¤å¾ˆæ—©å°±æˆäº¤äº†ï¼Œ500wå¼€å§‹ï¼Œ490wå°±ç»“æŸäº†ï¼Œå¯¹äºæ‹å–æ–¹æ¥è¯´ï¼Œæ”¶ç›Šå¤§ã€‚

### 8.2.2 Risk Seeking

æ„¿æ„é•¿æ—¶é—´ç­‰å¾…æ—¶æœºï¼Œå¢å¤§é£é™©ï¼Œå¢é«˜æ”¶ç›Š

Risk Seekingæ„¿æ„é•¿æ—¶é—´çš„ç­‰å¾…ï¼Œå› æ­¤å¾ˆæ™šæ‰æˆäº¤ï¼Œ500wå¼€å§‹ï¼Œ50wæ‰ç»“æŸï¼Œå¯¹äºæ‹å–æ–¹è€Œè¨€ï¼Œæ”¶ç›Šå°‘ã€‚

## 8.3 First Price Sealed Bid Auction

æ‹å–åªæœ‰ä¸€è½®ï¼Œå–ç»™å‡ºä»·ä¿¡å°ä¸­æœ€é«˜çš„é‚£ä½ä¹°å®¶

ä¼˜åŠ¿ç­–ç•¥ï¼ˆdominate strategyï¼‰ï¼šä½äºçœŸæ­£ä»·æ ¼å«ä»·

## 8.4 Vickrey Auction

æ‹å–åªæœ‰ä¸€è½®ï¼Œå–ç»™å‡ºä»·ä¿¡å°ä¸­æœ€é«˜çš„é‚£ä½ä¹°å®¶ï¼Œä½†åªéœ€è¦ç”¨ç¬¬äºŒé«˜çš„ä»·æ ¼è´­ä¹°å³å¯

ä¼˜åŠ¿ç­–ç•¥ï¼ˆdominate strategyï¼‰ï¼š**å‡ºçœŸå®çš„ä¼°ä»·æ¥å«ä»·**

å¦‚æœå‡ºé«˜äºä¼°ä»·çš„ä»·æ ¼æ¥å«ä»·ï¼Œæœ‰å¯èƒ½ä¼šæ‹åˆ°ï¼Œä½†æ˜¯ä¼šä»˜å‡ºæ›´å¤šçš„ä»·æ ¼è´­ä¹°ï¼Œå³ä½¿ä¹°åˆ°ä¹Ÿä¼šæŸå¤±

å¦‚æœå‡ºä½äºä¼°ä»·çš„ä»·æ ¼æ¥å«ä»·ï¼Œå³ä¾¿æ‹å–åˆ°äº†è—å“ï¼Œä¹Ÿè¦ä»˜å‡ºç¬¬äºŒé«˜çš„ä»·æ ¼æ¥è´­ä¹°ï¼Œå‡ºä½ä»·æ²¡æœ‰å¾—åˆ°ç›¸åº”çš„å›æŠ¥

æ³¨æ„ï¼ŒVickrey æ‹å–ä¼šä½¿åç¤¾ä¼šè¡Œä¸ºæˆä¸ºå¯èƒ½ã€‚å‡è®¾ä½ å¸Œæœ›å¾—åˆ°æŸç‰©å“å¹¶ä¸”è‡ªå·±ä¼°ä»·ä¸º90ç¾å…ƒï¼Œä½†æ˜¯ï¼Œä½ çŸ¥é“æœ‰å…¶ä»– Agentä¹Ÿæƒ³å¾—åˆ°å®ƒå¹¶ä¸”ä¼°ä»·ä¸º 100 ç¾å…ƒã€‚å› ä¸ºç»™å‡ºçœŸæ­£æœ‰æ•ˆçš„ä¼˜åŠ¿ç­–ç•¥ï¼Œä½ ä¸å¯èƒ½æœ‰æ¯”90ç¾å…ƒæ›´å¥½çš„å‡ºä»·ï¼›ä½ çš„å¯¹æ‰‹å‡ºä»·100 ç¾å…ƒå¹¶ä¸”å¾—åˆ°äº†è¿™ä¸ªç‰©å“ï¼Œä½†æ˜¯åªè¦æ”¯ä»˜90 ç¾å…ƒã€‚å¯¹è¿™æ ·çš„ç»“æœä½ å¯èƒ½ä¸ä¼šé«˜å…´ï¼šä½ å¯èƒ½æƒ³è¦â€œæƒ©è¡Œâ€ä¸€ä¸‹è·èƒœçš„å¯¹æ‰‹ï¼Œåº”è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿå‡è®¾ä½ å‡ºä»·99 ç¾å…ƒè€Œä¸æ˜¯90 ç¾å…ƒï¼Œä½ ä»ç„¶ä¼šæŠŠè¿™ä¸ªç‰©å“è¾“ç»™ä½ çš„å¯¹æ‰‹-ä½†æ˜¯ä»–è¦æ¯”ä½ ä»¥å®Œå…¨çœŸæ­£çš„å«ä»·å¤šæ”¯ä»˜ï¼Œç¾å…ƒã€‚å½“ç„¶ï¼Œè¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½ å¿…é¡»å¯¹å¯¹æ‰‹çš„å¦‚ä½•å«ä»·éå¸¸è‡ªä¿¡-ä½ ä¸æƒ³å‡ºä»·99 ç¾å…ƒè€Œå‘ç°ä½ çš„å¯¹æ‰‹åªå‡ºä»·95 ç¾å…ƒï¼Œä½ è¦ä»¥æ¯”ä½ çš„ä¼°ä»·é«˜5ç¾å…ƒç•™ä¸‹è¿™ä¸ªç‰©å“ã€‚è¿™ç§è¡Œä¸ºä¼šåœ¨å•†ä¸šç¯å¢ƒä¸‹å‡ºç°ï¼Œå…¶ä¸­ä¸€ä¸ªå…¬å¸æ²¡æœ‰åŠæ³•ä¸å¦ä¸€ä¸ªå…¬å¸ç›´æ¥ç«äº‰ï¼Œä½†æ˜¯å¯ä»¥åˆ©ç”¨å®ƒä»¬çš„è§è§£æ¥è¿«ä½¿å¯¹æ‰‹ç ´äº§



## 8.5 Combinatorial Auctions

ç»„åˆæ‹å–

å¦‚æœæœ‰å¤šä¸ªèµ„æºï¼Œå¹¶ä¸”å¤šä¸ªèµ„æºä¹‹é—´å¯èƒ½å­˜åœ¨ä¾èµ–å…³ç³»ï¼Œé‚£ä¹ˆå¯¹äºé’çè¿™äº›èµ„æºçš„ç”¨æˆ·è€Œè¨€ï¼Œè´­ä¹°å¤šä¸ªç›¸äº’å…³è”çš„èµ„æºçš„æ€§ä»·æ¯”å¯èƒ½è¦ä¼˜äºå•ç‹¬çš„è´­ä¹°ä¸€ä¸ªèµ„æºï¼Œå¯¹äºå¹³å°è€Œè¨€çš„è¯ï¼Œä¹Ÿå¯èƒ½ä¼šåˆ©ç›Šæœ€å¤§åŒ–ã€‚é‚£ä¹ˆï¼Œå¦‚ä½•åˆ†é…è¿™ä¸€ç³»åˆ—ç›¸äº’å…³è”çš„èµ„æºå°±æ˜¯ä¸€ä¸ªé—®é¢˜äº†ï¼Œ**ç»„åˆæ‹å–**å°±è¯ç”Ÿäº†ï¼

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211231014721802.png" alt="image-20211231014721802" style="zoom:40%;" />

The allocation ({z2,z3},{z1}) gives the highest social welfare of:

ğ‘£1({ğ‘§2,ğ‘§3})+ğ‘£2({ğ‘§1})=6+5=11

## 8.6 Vickrey-Clarke-Groves mechanism(VCG)

- each agent submits a valuation function to the auctioneer (step 1 below), 
- the auctioneer works out the allocation that will maximise the social welfare (step 2 below) and makes this allocation (step 3 below),
- the amount each agent must pay is calculated as in step 4 below.

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211231015443539.png" alt="image-20211231015443539" style="zoom:40%;" />

$P_i$è§£é‡Šï¼šAgiå‚åŠ å‰- Agiå‚åŠ åçš„social warfare

Social welfare without i: å¦‚æœiç›´æ¥ä¸å‚ä¸ç«æ‹ï¼Œå³åªæœ‰å…¶ä»–ä»£ç†å‚ä¸çš„æƒ…å†µä¸‹ï¼Œæ‹å–è€…æ ¹æ®social welfareæœ€å¤§åŒ–ç»™å‡ºçš„æœ€ä¼˜åˆ†é…ï¼Œç®—å¾—æ²¡æœ‰içš„æƒ…å†µä¸‹çš„å’Œ

Toal value to other agentsï¼šæ‰€æœ‰ä»£ç†å‚ä¸åˆ°ç«æ‹ä¸­ï¼Œæ‹å–è€…æ ¹æ®social welfareæœ€å¤§åŒ–ç»™å‡ºçš„æœ€ä¼˜åˆ†é…ã€‚**åˆ†é…åå°†içš„æŠ¥ä»·å¿½ç•¥ï¼ˆè®¾ä¸º0ï¼‰ä¹‹åè®¡ç®—çš„ä»·å€¼å’Œ**



E.g:

Suppose we have two goods a and b and two agents ğ‘ğ‘”1ag1 and ğ‘ğ‘”2ag2 who can each obtain at most one item. The agents declare the following true valuation functions:

ğ‘£Ì‚ 1({ğ‘})=ğ‘£1({ğ‘})=4

ğ‘£Ì‚ 1({ğ‘})=ğ‘£1({ğ‘})=5

ğ‘£Ì‚ 2({ğ‘})=ğ‘£2({ğ‘})=7

ğ‘£Ì‚ 2({ğ‘})=ğ‘£2({ğ‘})=9

- Which allocation of goods is made?

   æœ€å¤§åŒ–social welfareåŸåˆ™æ¥è®²ï¼Œæ–¹æ¡ˆ({a},{b}) æœ€ä½³ï¼Œaå–ç»™ag1ï¼Œbå–ç»™ag2

- How much does each agent pay to the mechanism?

â€‹		å‡è®¾Ag1æ²¡å‚ä¸ï¼Œåˆ™åº”åˆ†é…({},{b})æ¥æ»¡è¶³æœ€å¤§åŒ–sw; å‡è®¾Ag2æ²¡å‚ä¸ï¼Œåˆ™åº”åˆ†é…({b},{})æ¥æ»¡è¶³æœ€å¤§åŒ–sw

â€‹		Ag1 should pay = Ag1ä¸å‚ä¸ç«æ‹çš„æœ€å¤§swä¹‹å’Œ - éƒ½å‚ä¸ç«æ‹å¹¶å¿½ç•¥Ag1çš„æœ€å¤§swä¹‹å’Œ = ({},{b}) - ({a},{b})å¿½ç•¥{a} = 9 - 9 =0ï¼›

â€‹		Ag2 should pay = Ag2ä¸å‚ä¸ç«æ‹çš„æœ€å¤§swä¹‹å’Œ - éƒ½å‚ä¸ç«æ‹å¹¶å¿½ç•¥Ag2çš„æœ€å¤§swä¹‹å’Œ = ({b},{}) - ({a},{b})å¿½ç•¥{b} = 5 - 4 =1;

- What utility does each agent get?

  Ag1 utility = æœ€ä½³æ–¹æ¡ˆ({a},{b}) ä¸­Ag1åº”ä»˜çš„ - Ag1 åº”ä»˜çš„ = 4 - 0 = 0ï¼›

  Ag2 utility = æœ€ä½³æ–¹æ¡ˆ({a},{b}) ä¸­Ag2åº”ä»˜çš„ - Ag2 åº”ä»˜çš„ = 9 - 1 = 8ï¼›

- Can agent ag1 improve its utility by lying and declaring valuation ğ‘£Ì‚ 1â‰ ğ‘£1? Hint: think about the possible cases.

â€‹		Case 1: ({a},{b})è®¡ç®—utility = 4

â€‹		Case 2: ({b},{a})è®¡ç®—utility = 3

â€‹		æ‰€ä»¥ä¸å¯èƒ½



Like with the Vickrey auctions, the **dominant strategy** for the VCG mechanism is to submit your true valuation at step 1. This is a desirable property, since it means that the bidders do not need to expend any effort in reasoning about what valuation to submit, they can do no better than to submit their true valuation.

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211231030617352.png" alt="image-20211231030617352" style="zoom:50%;" />

aå’Œbåªèƒ½åˆ†é…ç»™ä¸€ä¸ªäººï¼Œ{a}{b}



# Week9: Reasoning with Arguments

An argument has a claim and a support. (Sometimes we refer to the claim as the argument's conclusion, and sometimes we refer to the support as the argument's premises.) The support of an argument provides reasons to believe in the claim. 

- Claimï¼šç»“è®º
- Support: å‰æ
-  Support is equal to the claim called **atomic arguments**, æ¯”å¦‚âŸ¨{ğ‘},ğ‘âŸ©ï¼Œsupport:{a}; Claim: a

- When the claim of an argument A contradicts an argument B we say that A **attacks** B.
  - **Rebuttal**: this is when the claim of an argument ğ‘ğ‘¥ contradicts the claim of an argument ğ‘ğ‘¦, and so leads to a symmetric attack (each argument attacks the other).
  - **Undercut**: this is when the claim of an argument ğ‘ğ‘¥ contradicts (some part of) the support of an argument ğ‘ğ‘¦, leading to a one way attack from ğ‘ğ‘¥ to ğ‘ğ‘¦.
  - è®ºæ®ä¹‹é—´çš„æ”»å‡»æ€§ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬å†³å®šå“ªäº›è®ºè¯çš„å­é›†æ˜¯åˆç†çš„ï¼Œå¯ä»¥é›†ä½“æ¥å—çš„

Let Î” be a database of logic formulae. An **argument** constructed from Î” is a tuple âŸ¨Î¦,ğœ™âŸ© such that:

- Î¦âŠ†Î”, è®ºç‚¹å¿…é¡»æ¥è‡ªæ•°æ®åº“
- Î¦âŠ¬âŠ¥, 
- Î¦âŠ¢ğœ™, and ï¼ˆæ”¯æŒå¿…é¡»åŒ…å«ä¸»å¼ ï¼‰
- there is no Î¦â€²âŠŠÎ¦ such that Î¦â€²âŠ¢ğœ™. 

What this definition says is that:

- the support of the argument must come from your database,
- the support must be consistent,
- the support must entail the claim, and 
- the support is a minimal set (under set inclusion) that entails the claim.

For an argument ğ‘ğ‘Ÿğ‘”ğ‘¥=âŸ¨Î¦,ğœ™âŸ©, we often refer to the claim of ğ‘ğ‘Ÿğ‘”ğ‘¥ as ğ‘ğ‘™ğ‘ğ‘–ğ‘š(ğ‘ğ‘Ÿğ‘”ğ‘¥)=ğœ™, and refer to the support of ğ‘ğ‘Ÿğ‘”ğ‘¥ as ğ‘ ğ‘¢ğ‘ğ‘ğ‘œğ‘Ÿğ‘¡(ğ‘ğ‘Ÿğ‘”ğ‘¥)=Î¦.

## 9.1 Attacks Between Arguments

 Two types of attacks: rebuttals and undercuts

Let âŸ¨Î¦1,ğœ™1âŸ© and âŸ¨Î¦2,ğœ™2âŸ© be arguments.

- âŸ¨Î¦1,ğœ™1âŸ© **rebutsåé©³** âŸ¨Î¦2,ğœ™2âŸ© if and only if ğœ™1â‰¡Â¬ğœ™2

- âŸ¨Î¦1,ğœ™1âŸ© **undercutså‰Šå¼±** âŸ¨Î¦2,ğœ™2âŸ© if and only if there is some ğœ™âˆˆÎ¦2 such that ğœ™1â‰¡Â¬ğœ™

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20211231170605039.png" alt="image-20211231170605039" style="zoom:40%;" />

## 9.2 Argument Evaluation

### 9.2.1 abstract argumentation graph

- å½“ä¸”ä»…å½“ä¸€ä¸ªè®ºç‚¹çš„**æ‰€æœ‰**æ”»å‡»è€…éƒ½è¢«æ ‡è®°ä¸ºOUTæ—¶ï¼Œè¯¥è®ºç‚¹æ‰è¢«æ ‡è®°ä¸ºINã€‚1âƒ£ï¸

- å½“ä¸”ä»…å½“ä¸€ä¸ªè®ºæ®**è‡³å°‘æœ‰ä¸€ä¸ª**æ”»å‡»è€…è¢«æ ‡è®°ä¸ºINæ—¶ï¼Œè¯¥è®ºæ®æ‰è¢«æ ‡è®°ä¸ºOUTã€‚2âƒ£ï¸
  - è®ºè¯hæ²¡æœ‰æ”»å‡»è€…ï¼Œå› æ­¤æ˜¾ç„¶æ˜¯å¯æ¥å—çš„ï¼ˆINï¼‰
  - å› ä¸ºhæ˜¯å¯æ¥å—çš„ï¼Œå¹¶ä¸”hæ”»å‡»aï¼Œå› æ­¤aæ˜¯ä¸å¯æ¥å—çš„è®ºè¯ï¼Œç§°ä¸ºOUT
  - åŒæ ·ï¼Œå› ä¸ºhæ˜¯å¯æ¥å—çš„ï¼Œå¹¶ä¸”hæ”»å‡»pï¼Œå› æ­¤pæ˜¯ä¸å¯æ¥å—çš„OUT
  - å› ä¸ºpæ˜¯ä¸å¯æ¥å—çš„OUTï¼Œå®ƒæ˜¯qçš„å”¯ä¸€æ”»å‡»è€…ï¼Œå› æ­¤qæ˜¯å¯æ¥å—çš„IN
- æ»¡è¶³1âƒ£ï¸å’Œ2âƒ£ï¸çš„è¢«ç§°ä¸ºCOMPLETE LABELING

Different **argumentation semantics** place different constraints on which of the complete labellings are valid. 

- Under the **complete semantics**, any complete labelling is valid. ä»»ä½•complete labellingéƒ½æ˜¯complete semanticsï¼ŒåŒ…æ‹¬grounded semanticså’Œpreferred semanticsä¹Ÿæ˜¯complete semantics

- Under the **grounded semantics**, only the complete labelling that maximises (with reference to set inclusion) the arguments that are labelled as UNDEC is valid. (Note, it is guaranteed that there is always exactly one such labelling.) è®©UNDECè¿‘å¯èƒ½å¤šæ ‡ï¼ŒINä»…å¯èƒ½çš„å°‘
  - An argument is **acceptable under the grounded semantics** if and only if it is labelled as IN under the grounded semantics. 

- Under the **preferred semantics**, any complete labelling that maximises (with reference to set inclusion) the arguments that are labelled as IN are valid. (Note, there may be more than one such labelling.) è®©INè¿‘å¯èƒ½çš„å¤šæ ‡
  - Credulouså¯ä¿¡çš„ï¼šå‡ºç°è¿‡ä¸€æ¬¡INå°±å¯æ¥å—
  - Scepticalæ€€ç–‘çš„ï¼šå…¨ä¸ºINæ‰å¯æ¥å—

E.g:

<img src="/Users/kevin/Library/Application Support/typora-user-images/image-20220101204626402.png" alt="image-20220101204626402" style="zoom:40%;" />

Grounded Accp: a1

![image-20220101204722745](/Users/kevin/Library/Application Support/typora-user-images/image-20220101204722745.png)

### 9.2.2 Why argue?

è®ºè¯æ”¯æŒæ‰€è°“çš„éå•è°ƒæ€§åŸç†ï¼ˆnon-monotonic reasoningï¼‰,æ•°æ®åº“çŸ¥è¯†çš„å¢é•¿å¯èƒ½**å¢åŠ ç»“è®º**ï¼Œä¹Ÿå¯èƒ½**æ’¤å›ç»“è®º**ã€‚ç»å…¸æ¨ç†åªæ”¯æŒå•è°ƒæ¨ç†ï¼Œå³éšç€çŸ¥è¯†åº“çš„å¢é•¿ï¼Œä¸ä¼šæ’¤å›ç»“è®ºã€‚è®ºè¯ä¹Ÿå…è®¸Agentåœ¨è·å¾—æ–°çŸ¥è¯†æ—¶ï¼Œæ”¹å˜ä»–ä»¬å¯¹äº‹ç‰©çš„ç«‹åœºï¼ˆç»å…¸æ¨ç†ä¸å…è®¸ï¼‰



# Week10: Argumentation-based Dialogues

When reasoning with argumentation, as an agent gains more knowledge it is possible that it may [retract] conclusions as well as add them. This is called [nonâ€‘monotonic] reasoning and means that the agent can change its position on things. å½“ç”¨è®ºè¯æ¨ç†æ—¶ï¼Œéšç€ä»£ç†äººè·å¾—æ›´å¤šçš„çŸ¥è¯†ï¼Œå®ƒæœ‰å¯èƒ½[æ”¶å›]ç»“è®ºï¼Œä¹Ÿå¯èƒ½å¢åŠ ç»“è®ºã€‚è¿™è¢«ç§°ä¸º[éå•è°ƒæ€§]æ¨ç†ï¼Œæ„å‘³ç€ä»£ç†äººå¯ä»¥æ”¹å˜å…¶å¯¹äº‹ç‰©çš„ç«‹åœºã€‚

We can use subjective [preferences] to resolve two way (symmetric) attacks in argumentation.

Argumentative principles are familiar to humans. This makes argumentation well-suited to supporting [explainable] AI.

An argument dialogue [protocol] defines the space of permissible dialogues; an agent uses its private [strategy] to determine which permissible move to make at any point. 

A dialogue of type [inquiry] allows the agents to jointly reason.

A dialogue of type [deliberation] is appropriate when the agents need to reach a decision about what to do.

An agent can use its opponent model to strategise in argument dialogues. An opponent model is typically [uncertain] since agents' internal states are private. 
